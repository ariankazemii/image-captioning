{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "directory = 'D:\\CS_Internship\\Exercises\\Image Captioning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(directory, 'captions.txt'), 'r') as f:\n",
    "    captions_lines = f.readlines()[1:]\n",
    "    captions_file = ''.join(captions_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of Images and their captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captions = {}\n",
    "\n",
    "for line in captions_file.split('\\n'):\n",
    "    tokens = line.split(',')\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "\n",
    "    image_id, *caption_tokens = tokens\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption_tokens).strip()\n",
    "\n",
    "    image_captions.setdefault(image_id, []).append(caption)\n",
    "\n",
    "image_numbers = len(image_captions) \n",
    "print(\"Image Numbers:\", image_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    words = re.findall(r'[a-zA-Z]+', text.lower())\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    cleaned_text = \"begin \" + \" \".join(words) + \" end\"\n",
    "    return cleaned_text\n",
    "\n",
    "for key, captions in image_captions.items():\n",
    "    image_captions[key] = [clean_text(caption) for caption in captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = [caption for key in image_captions for caption in image_captions[key]]\n",
    "count_captions = len(all_captions)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "\n",
    "count_captions, vocab_size , max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16()\n",
    "base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n",
    "\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = {}\n",
    "image_directory= os.path.join(directory, 'Images')\n",
    "\n",
    "for img_name in tqdm(os.listdir(image_directory)):\n",
    "    img_path = os.path.join(image_directory, img_name)\n",
    "    \n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    feature = base_model.predict(image)\n",
    "    \n",
    "    image_id = img_name.split('.')[0]\n",
    "    image_features[image_id] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(image_features, os.path.join(directory, 'Extracted_Features.joblib'))\n",
    "image_features = joblib.load(os.path.join(directory, 'Extracted_Features.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_ids = list(image_captions.keys())\n",
    "train, test = train_test_split(image_ids, test_size=0.10, random_state=42)\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate, build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_data(data_keys, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    while True:\n",
    "        selected_keys = np.random.choice(data_keys, size=batch_size, replace=True)\n",
    "        X1, X2, y = [], [], []\n",
    "\n",
    "        for key in selected_keys:\n",
    "            captions = image_captions[key]\n",
    "            selected_caption = np.random.choice(captions)\n",
    "\n",
    "            seq = tokenizer.texts_to_sequences([selected_caption])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(image_features[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "        yield [np.array(X1), np.array(X2)], np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_length, vocab_size):\n",
    "    image_input = Input(shape=(4096,))\n",
    "    text_input = Input(shape=(max_length,))\n",
    "\n",
    "    i1 = Dense(256, activation='relu')(Dropout(0.4)(image_input))\n",
    "    t1 = LSTM(256)(Dropout(0.4)(Embedding(vocab_size, 256, mask_zero=True)(text_input)))\n",
    "\n",
    "    decoder = Dense(256, activation='relu')(add([i1, t1]))\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs=[image_input, text_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size, epochs):\n",
    "    steps_per_epoch = len(train_data) // batch_size\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        generator = generator_data(train_data, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size)\n",
    "        model.fit(generator, epochs= 1, steps_per_epoch= steps_per_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply functions and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = build_model(max_length, vocab_size)\n",
    "train_model(model, train, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(directory+'/image_captioning_model20_32.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Generate Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    \n",
    "    in_text = 'begin:'\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        y = model.predict([image, sequence])\n",
    "        y = np.argmax(y)\n",
    "        word = idx_to_word(y, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == 'end':\n",
    "            break  \n",
    "    return in_text\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    img_path = os.path.join(directory+\"\\images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    captions = image_captions[image_id]\n",
    "    print('Real Captions:')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "\n",
    "    y_pred = predict_caption(model, image_features[image_id], tokenizer, max_length)\n",
    "    print('Predicted Caption:','\\n', y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
